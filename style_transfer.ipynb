{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"构建VGG-16网络结构， 读取模型参数\"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"构建卷积层\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1, 1, 1, 1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "    \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"构建池化层\"\"\"\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"构建全连接层\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    \n",
    "    def flatten_layer(self, x, name):\n",
    "        with tf.name_scope(name):\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "        \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"构建VGG16网络结构\n",
    "        parameters:\n",
    "        - x_rgb: [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print('构建开始 building model ...')\n",
    "        \n",
    "        r, g, b = tf.split(x_rgb, [1, 1, 1], axis=3)\n",
    "        x_bgr = tf.concat([b - VGG_MEAN[0], \n",
    "                           g - VGG_MEAN[1], \n",
    "                           r - VGG_MEAN[2]], \n",
    "                         axis=3)\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(x_bgr, b'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, b'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, b'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, b'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, b'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, b'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, b'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, b'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, b'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, b'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, b'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, b'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, b'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten5')\n",
    "        \n",
    "        self.fc6 = self.fc_layer(self.flatten5, b'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, b'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, b'fc8', activation=None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "        \n",
    "        print('构建结束 building model finished: %4ds' % (time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata_dict = np.load('vgg16.npy',encoding='bytes').item()\\n\\nvgg16_for_result = VGGNet(data_dict)\\ncontent = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\\nvgg16_for_result.build(content)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_npy_pyth = 'vgg16.npy'\n",
    "content_img_path = 'timg.jpg'\n",
    "style_img_path = 'timg1.jpg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = './run_style_transfer'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\"\"\"\n",
    "data_dict = np.load('vgg16.npy',encoding='bytes').item()\n",
    "\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "vgg16_for_result.build(content)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建开始 building model ...\n",
      "构建结束 building model finished:    3s\n",
      "构建开始 building model ...\n",
      "构建结束 building model finished:    3s\n",
      "构建开始 building model ...\n",
      "构建结束 building model finished:    4s\n"
     ]
    }
   ],
   "source": [
    "def initial_result(shape, mean, stddev):\n",
    "    \"\"\"生成初始化图像\"\"\"\n",
    "    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calulates gram matrix\n",
    "    Args:\n",
    "    - x: feaures extracted from VGG Net. shape: [1, width, height, ch]\n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, w * h, ch])\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]\n",
    "    gram = tf.matmul(features, features, adjoint_a=True) / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "\"\"\"提取特征\"\"\"\n",
    "data_dict = np.load('vgg16.npy',encoding='bytes').item()\n",
    "vgg16_for_content = VGGNet(data_dict)\n",
    "vgg16_for_style = VGGNet(data_dict)\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg16_for_content.build(content)\n",
    "vgg16_for_style.build(style)\n",
    "vgg16_for_result.build(result)\n",
    "\n",
    "content_features = [vgg16_for_content.conv1_1, \n",
    "#                    vgg16_for_content.conv2_2, \n",
    "#                    vgg16_for_content.conv3_3, \n",
    "#                    vgg16_for_content.conv4_3, \n",
    "#                    vgg16_for_content.con5_3\n",
    "                   ]\n",
    "result_content_features = [vgg16_for_result.conv1_1, \n",
    "#                    vgg16_for_result.conv2_2, \n",
    "#                    vgg16_for_result.conv3_3, \n",
    "#                    vgg16_for_result.conv4_3, \n",
    "#                    vgg16_for_result.con5_3\n",
    "                   ]\n",
    "style_features = [# vgg16_for_style.conv1_1, \n",
    "#                    vgg16_for_style.conv2_2, \n",
    "#                    vgg16_for_style.conv3_3, \n",
    "                   vgg16_for_style.conv4_3, \n",
    "#                    vgg16_for_style.con5_3\n",
    "                   ]\n",
    "style_gram =  [gram_matrix(features) for features in style_features]\n",
    "\n",
    "result_style_features = [\n",
    "    # vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg16_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "result_style_gram = [gram_matrix(features) for features in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1, 2], [3, 4], zip([1,2], [3,4]) -> [(1, 3), (2, 4)]\n",
    "# shape: [1, width, height, channel]\n",
    "\"\"\"内容损失\"\"\"\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "    \n",
    "\"\"\"风格损失\"\"\"\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "    \n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 5100.2485, content_loss: 3020.9983, style_loss:   9.5963\n",
      "step: 2, loss_value: 4314.9600, content_loss: 2979.8660, style_loss:   8.0339\n",
      "step: 3, loss_value: 4260.1289, content_loss: 3029.2920, style_loss:   7.9144\n",
      "step: 4, loss_value: 3754.5806, content_loss: 3076.5542, style_loss:   6.8939\n",
      "step: 5, loss_value: 3214.1277, content_loss: 3176.8845, style_loss:   5.7929\n",
      "step: 6, loss_value: 2687.6665, content_loss: 3314.3989, style_loss:   4.7125\n",
      "step: 7, loss_value: 2420.1741, content_loss: 3468.2639, style_loss:   4.1467\n",
      "step: 8, loss_value: 2097.0728, content_loss: 3620.7212, style_loss:   3.4700\n",
      "step: 9, loss_value: 1909.9512, content_loss: 3782.6716, style_loss:   3.0634\n",
      "step: 10, loss_value: 1804.8783, content_loss: 3949.8228, style_loss:   2.8198\n",
      "step: 11, loss_value: 1668.9946, content_loss: 4107.7329, style_loss:   2.5164\n",
      "step: 12, loss_value: 1580.4730, content_loss: 4259.1909, style_loss:   2.3091\n",
      "step: 13, loss_value: 1494.9049, content_loss: 4407.3350, style_loss:   2.1083\n",
      "step: 14, loss_value: 1448.2673, content_loss: 4547.9009, style_loss:   1.9870\n",
      "step: 15, loss_value: 1395.4857, content_loss: 4674.5293, style_loss:   1.8561\n",
      "step: 16, loss_value: 1354.1327, content_loss: 4793.2827, style_loss:   1.7496\n",
      "step: 17, loss_value: 1318.3508, content_loss: 4903.5435, style_loss:   1.6560\n",
      "step: 18, loss_value: 1284.3090, content_loss: 4998.7495, style_loss:   1.5689\n",
      "step: 19, loss_value: 1262.4922, content_loss: 5081.9907, style_loss:   1.5086\n",
      "step: 20, loss_value: 1236.8792, content_loss: 5155.8823, style_loss:   1.4426\n",
      "step: 21, loss_value: 1215.5886, content_loss: 5216.4067, style_loss:   1.3879\n",
      "step: 22, loss_value: 1194.8091, content_loss: 5263.5161, style_loss:   1.3369\n",
      "step: 23, loss_value: 1173.0387, content_loss: 5301.6299, style_loss:   1.2858\n",
      "step: 24, loss_value: 1156.3638, content_loss: 5330.3198, style_loss:   1.2467\n",
      "step: 25, loss_value: 1136.6930, content_loss: 5347.7515, style_loss:   1.2038\n",
      "step: 26, loss_value: 1118.6433, content_loss: 5357.1499, style_loss:   1.1659\n",
      "step: 27, loss_value: 1102.0706, content_loss: 5357.6582, style_loss:   1.1326\n",
      "step: 28, loss_value: 1084.0132, content_loss: 5348.1768, style_loss:   1.0984\n",
      "step: 29, loss_value: 1066.8164, content_loss: 5332.7144, style_loss:   1.0671\n",
      "step: 30, loss_value: 1051.0869, content_loss: 5311.4351, style_loss:   1.0399\n",
      "step: 31, loss_value: 1032.3315, content_loss: 5281.9829, style_loss:   1.0083\n",
      "step: 32, loss_value: 1014.0122, content_loss: 5247.7847, style_loss:   0.9785\n",
      "step: 33, loss_value: 997.8065, content_loss: 5208.9424, style_loss:   0.9538\n",
      "step: 34, loss_value: 980.5455, content_loss: 5163.6782, style_loss:   0.9284\n",
      "step: 35, loss_value: 963.3528, content_loss: 5116.0171, style_loss:   0.9035\n",
      "step: 36, loss_value: 947.4789, content_loss: 5064.3955, style_loss:   0.8821\n",
      "step: 37, loss_value: 932.9209, content_loss: 5009.4004, style_loss:   0.8640\n",
      "step: 38, loss_value: 917.0669, content_loss: 4953.6143, style_loss:   0.8434\n",
      "step: 39, loss_value: 901.1279, content_loss: 4893.8398, style_loss:   0.8235\n",
      "step: 40, loss_value: 885.2064, content_loss: 4833.8486, style_loss:   0.8036\n",
      "step: 41, loss_value: 869.8355, content_loss: 4771.6128, style_loss:   0.7853\n",
      "step: 42, loss_value: 855.7140, content_loss: 4708.1270, style_loss:   0.7698\n",
      "step: 43, loss_value: 842.0112, content_loss: 4645.6567, style_loss:   0.7549\n",
      "step: 44, loss_value: 828.2549, content_loss: 4581.3892, style_loss:   0.7402\n",
      "step: 45, loss_value: 814.2183, content_loss: 4519.2534, style_loss:   0.7246\n",
      "step: 46, loss_value: 801.0557, content_loss: 4454.6201, style_loss:   0.7112\n",
      "step: 47, loss_value: 788.6000, content_loss: 4393.0952, style_loss:   0.6986\n",
      "step: 48, loss_value: 778.1241, content_loss: 4328.2598, style_loss:   0.6906\n",
      "step: 49, loss_value: 767.7185, content_loss: 4268.6660, style_loss:   0.6817\n",
      "step: 50, loss_value: 762.3535, content_loss: 4204.6738, style_loss:   0.6838\n",
      "step: 51, loss_value: 748.8037, content_loss: 4148.0317, style_loss:   0.6680\n",
      "step: 52, loss_value: 736.6975, content_loss: 4085.8523, style_loss:   0.6562\n",
      "step: 53, loss_value: 717.0367, content_loss: 4030.6431, style_loss:   0.6279\n",
      "step: 54, loss_value: 702.9915, content_loss: 3972.9268, style_loss:   0.6114\n",
      "step: 55, loss_value: 694.6628, content_loss: 3916.3445, style_loss:   0.6061\n",
      "step: 56, loss_value: 689.8601, content_loss: 3863.3596, style_loss:   0.6070\n",
      "step: 57, loss_value: 689.1970, content_loss: 3806.8000, style_loss:   0.6170\n",
      "step: 58, loss_value: 679.2961, content_loss: 3758.4709, style_loss:   0.6069\n",
      "step: 59, loss_value: 671.7313, content_loss: 3703.9502, style_loss:   0.6027\n",
      "step: 60, loss_value: 650.4860, content_loss: 3657.4561, style_loss:   0.5695\n",
      "step: 61, loss_value: 636.4574, content_loss: 3607.7805, style_loss:   0.5514\n",
      "step: 62, loss_value: 631.7189, content_loss: 3558.9746, style_loss:   0.5516\n",
      "step: 63, loss_value: 629.0080, content_loss: 3514.0779, style_loss:   0.5552\n",
      "step: 64, loss_value: 630.3088, content_loss: 3464.6240, style_loss:   0.5677\n",
      "step: 65, loss_value: 618.5219, content_loss: 3423.8042, style_loss:   0.5523\n",
      "step: 66, loss_value: 607.1367, content_loss: 3376.9419, style_loss:   0.5389\n",
      "step: 67, loss_value: 590.5588, content_loss: 3336.4309, style_loss:   0.5138\n",
      "step: 68, loss_value: 582.9855, content_loss: 3294.9114, style_loss:   0.5070\n",
      "step: 69, loss_value: 582.5809, content_loss: 3252.5908, style_loss:   0.5146\n",
      "step: 70, loss_value: 581.1826, content_loss: 3215.9927, style_loss:   0.5192\n",
      "step: 71, loss_value: 585.3741, content_loss: 3173.2837, style_loss:   0.5361\n",
      "step: 72, loss_value: 572.6349, content_loss: 3140.4268, style_loss:   0.5172\n",
      "step: 73, loss_value: 560.3904, content_loss: 3100.4343, style_loss:   0.5007\n",
      "step: 74, loss_value: 542.6578, content_loss: 3067.1562, style_loss:   0.4719\n",
      "step: 75, loss_value: 541.9464, content_loss: 3033.3230, style_loss:   0.4772\n",
      "step: 76, loss_value: 546.9589, content_loss: 2996.2800, style_loss:   0.4947\n",
      "step: 77, loss_value: 537.2220, content_loss: 2967.0471, style_loss:   0.4810\n",
      "step: 78, loss_value: 534.8683, content_loss: 2932.0122, style_loss:   0.4833\n",
      "step: 79, loss_value: 522.9316, content_loss: 2902.2278, style_loss:   0.4654\n",
      "step: 80, loss_value: 514.6608, content_loss: 2870.4836, style_loss:   0.4552\n",
      "step: 81, loss_value: 506.7775, content_loss: 2841.3457, style_loss:   0.4453\n",
      "step: 82, loss_value: 501.3593, content_loss: 2812.0645, style_loss:   0.4403\n",
      "step: 83, loss_value: 499.0848, content_loss: 2781.9810, style_loss:   0.4418\n",
      "step: 84, loss_value: 497.2327, content_loss: 2755.0613, style_loss:   0.4435\n",
      "step: 85, loss_value: 509.2455, content_loss: 2724.1045, style_loss:   0.4737\n",
      "step: 86, loss_value: 535.7195, content_loss: 2702.5823, style_loss:   0.5309\n",
      "step: 87, loss_value: 619.1252, content_loss: 2670.7646, style_loss:   0.7041\n",
      "step: 88, loss_value: 498.6586, content_loss: 2656.0078, style_loss:   0.4661\n",
      "step: 89, loss_value: 524.4482, content_loss: 2635.8159, style_loss:   0.5217\n",
      "step: 90, loss_value: 538.3525, content_loss: 2611.3147, style_loss:   0.5544\n",
      "step: 91, loss_value: 509.7472, content_loss: 2598.0845, style_loss:   0.4999\n",
      "step: 92, loss_value: 526.2979, content_loss: 2583.5251, style_loss:   0.5359\n",
      "step: 93, loss_value: 495.4578, content_loss: 2564.7783, style_loss:   0.4780\n",
      "step: 94, loss_value: 502.4287, content_loss: 2551.0894, style_loss:   0.4946\n",
      "step: 95, loss_value: 482.3197, content_loss: 2538.5554, style_loss:   0.4569\n",
      "step: 96, loss_value: 481.8359, content_loss: 2524.5764, style_loss:   0.4588\n",
      "step: 97, loss_value: 472.1894, content_loss: 2509.9341, style_loss:   0.4424\n",
      "step: 98, loss_value: 467.4148, content_loss: 2494.9392, style_loss:   0.4358\n",
      "step: 99, loss_value: 462.4281, content_loss: 2481.0938, style_loss:   0.4286\n",
      "step: 100, loss_value: 455.6931, content_loss: 2467.3196, style_loss:   0.4179\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content: content_val,\n",
    "                         style: style_val,\n",
    "                     })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \n",
    "              % (step+1,\n",
    "                 loss_value[0],\n",
    "                 content_loss_value[0],\n",
    "                 style_loss_value[0]))\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step+1))\n",
    "        result_val = result.eval(sess)[0]\n",
    "        result_val = np.clip(result_val, 0, 255)\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)\n",
    "        img.save(result_img_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
